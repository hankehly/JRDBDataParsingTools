{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0803387-a429-46e9-8804-1d116d719ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import datetime\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from JRDBDataParsingTools.data_schema import load_schema, create_pyspark_schema\n",
    "from JRDBDataParsingTools.data_parser import parse_line\n",
    "from JRDBDataParsingTools.file_downloader import download_and_extract_files\n",
    "from JRDBDataParsingTools.structured_logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3e2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5f359",
   "metadata": {},
   "source": [
    "# Download files from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b21435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JRDB credentials\n",
    "username = os.getenv(\"JRDB_USERNAME\")\n",
    "password = os.getenv(\"JRDB_PASSWORD\")\n",
    "# The directory where you want to download the files\n",
    "# Must be an absolute path\n",
    "download_dir = \"/Users/hankehly/Projects/JRDBDataParsingTools/downloads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8096d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"Downloading and extracting files from http://www.jrdb.com/member/datazip/Kab/index.html\", \"level\": \"info\", \"timestamp\": \"2023-12-30T19:00:47.265760Z\", \"logger\": \"JRDBDataParsingTools.file_downloader\"}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'datetime.date' object has no attribute 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m target_dataset_urls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Taken from http://www.jrdb.com/member/dataindex.html\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Comment out the ones you don't want to download.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.jrdb.com/member/datazip/Hjc/index.html\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m webpage_url \u001b[38;5;129;01min\u001b[39;00m target_dataset_urls:\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mdownload_and_extract_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwebpage_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/JRDBDataParsingTools/JRDBDataParsingTools/file_downloader.py:159\u001b[0m, in \u001b[0;36mdownload_and_extract_files\u001b[0;34m(webpage_url, username, password, download_dir, skip_year_files, threads, start_date, end_date)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_date_file(link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    158\u001b[0m     date \u001b[38;5;241m=\u001b[39m extract_date(file_link)\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_date \u001b[38;5;129;01mand\u001b[39;00m date \u001b[38;5;241m<\u001b[39m \u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m():\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_date \u001b[38;5;129;01mand\u001b[39;00m date \u001b[38;5;241m>\u001b[39m end_date\u001b[38;5;241m.\u001b[39mdate():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'datetime.date' object has no attribute 'date'"
     ]
    }
   ],
   "source": [
    "target_dataset_urls = [\n",
    "    # Taken from http://www.jrdb.com/member/dataindex.html\n",
    "    # Comment out the ones you don't want to download.\n",
    "    # Downloading all of them will take about ?\n",
    "    \"http://www.jrdb.com/member/datazip/Kab/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Bac/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Kyi/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Ukc/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Oz/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Oz/index2.html\",  # OW data\n",
    "    # \"http://www.jrdb.com/member/datazip/Ou/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Ot/index.html\",\n",
    "    # \"http://www.jrdb.com/member/datazip/Ov/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Cha/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Sed/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Skb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Tyb/index.html\",\n",
    "    \"http://www.jrdb.com/member/datazip/Hjc/index.html\",\n",
    "]\n",
    "\n",
    "for webpage_url in target_dataset_urls:\n",
    "    download_and_extract_files(\n",
    "        webpage_url, username, password, download_dir, start_date=datetime.date(2023, 12, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d318",
   "metadata": {},
   "source": [
    "# Import data into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e64b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/30 08:23:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "def etl(spark, schema_path: str, data_path: str | List[str], dbtable: str, surrogate_key_name: str):\n",
    "    logger.info(f\"Processing dataset {dbtable}\")\n",
    "    schema = load_schema(schema_path)\n",
    "    logger.info(\"Creating PySpark DataFrame\")\n",
    "    df = (\n",
    "        spark.read.format(\"binaryFile\")\n",
    "        .load(data_path)\n",
    "        .select(\"content\")\n",
    "        .rdd.flatMap(lambda x: x[0].splitlines())\n",
    "        .map(functools.partial(parse_line, schema=schema))\n",
    "        .toDF(create_pyspark_schema(schema))\n",
    "        .withColumn(surrogate_key_name, f.monotonically_increasing_id())\n",
    "        # Returns the wrong file name..\n",
    "        # .withColumn(\"input_file_name\", f.input_file_name())\n",
    "    )\n",
    "    logger.info(\"Writing to data warehouse\")\n",
    "    (\n",
    "        df.write.mode(\"overwrite\")\n",
    "        .format(\"jdbc\")\n",
    "        .options(\n",
    "            url=\"jdbc:postgresql://localhost:5432/jrdb\",\n",
    "            user=\"admin\",\n",
    "            password=\"admin\",\n",
    "            driver=\"org.postgresql.Driver\",\n",
    "            dbtable=dbtable,\n",
    "        )\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"postgresql-42.7.1.jar\").getOrCreate()\n",
    "schema_name = \"jrdb_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62199ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"Processing dataset jrdb_raw.kab\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:38.373758Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:38.386084Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:42.525121Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.bac\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:45.556198Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:45.567359Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:49.701563Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.kyi\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:51.784192Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:51.815505Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:24:55.684302Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.ukc\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:30.936062Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:30.942477Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:34.989732Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.oz\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:43.984487Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:43.988191Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:47.595288Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.ow\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:51.114070Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:51.116958Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:54.655061Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.ou\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:57.270827Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:25:57.273614Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:00.569975Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.ot\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:04.812022Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:04.814761Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:08.212602Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.ov\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:17.298626Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:17.301389Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:26:20.434165Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.cyb\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:15.378275Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:15.396513Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:19.175863Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.cha\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:27.041817Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:27.049005Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:29.705375Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.skb\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:34.768028Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:34.774901Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:38.681744Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.hjc\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:57.556177Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:27:57.560865Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:28:01.513612Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Processing dataset jrdb_raw.sed\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:28:03.641042Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:28:03.660768Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:28:07.621932Z\", \"logger\": \"__main__\"}\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    \"KAB\",\n",
    "    \"BAC\",\n",
    "    \"KYI\",\n",
    "    \"UKC\",\n",
    "    \"OZ\",\n",
    "    \"OW\",\n",
    "    \"OU\",\n",
    "    \"OT\",\n",
    "    \"OV\",\n",
    "    \"CYB\",\n",
    "    \"CHA\",\n",
    "    \"SKB\",\n",
    "    \"HJC\",\n",
    "    \"SED\", # Run remove_sed_duplicates.sql after loading this dataset\n",
    "    # \"TYB\"  # TYB is a special case because the file names are not consistent\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    etl(\n",
    "        spark,\n",
    "        schema_path=f\"schemas/{dataset}.yaml\",\n",
    "        data_path=str(Path(download_dir).joinpath(f\"{dataset}*.txt\")),\n",
    "        dbtable=f\"{schema_name}.{dataset.lower()}\",\n",
    "        surrogate_key_name=f\"{dataset.lower()}_sk\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65098ce",
   "metadata": {},
   "source": [
    "# Handle edge cases in TYB files before loading into Postgres\n",
    "\n",
    "The following TYB file in the annual pack contains null byte characters. Its daily file counterpart does not, so we must replace it before the file can be processed.\n",
    "* TYB060121.txt\n",
    "\n",
    "Starting 2021-09-04, TYB files are duplicated in the annual pack. One file name contains a \"_t\" while the other does not. The daily file counterpart contains the same information as the annual pack file whose name does not contain a \"_t\" in it. In addition, some of the \"_t\" files contain null byte characters. The following files are affected. All files with \"_t\" in the name are ignored when parsing.\n",
    "* TYB210904_t.txt\n",
    "* TYB210905_t.txt\n",
    "* TYB210911_t.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c555fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"event\": \"Processing dataset jrdb_raw.tyb\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:30:37.597795Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Creating PySpark DataFrame\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:30:37.656163Z\", \"logger\": \"__main__\"}\n",
      "{\"event\": \"Writing to data warehouse\", \"level\": \"info\", \"timestamp\": \"2023-12-30T14:30:41.059260Z\", \"logger\": \"__main__\"}\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TYB is a special case because the file names are not consistent\n",
    "tyb_pattern = re.compile(r\"TYB\\d{6}\\.txt$\")\n",
    "tyb_files_glob = Path(download_dir).glob(\"TYB*.txt\")\n",
    "tyb_files = [str(file) for file in tyb_files_glob if tyb_pattern.match(file.name)]\n",
    "etl(\n",
    "    spark,\n",
    "    schema_path=\"schemas/TYB.yaml\",\n",
    "    data_path=tyb_files,\n",
    "    dbtable=f\"{schema_name}.tyb\",\n",
    "    surrogate_key_name=\"tyb_sk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7857759",
   "metadata": {},
   "source": [
    "# Convert codes to CSV format\n",
    "\n",
    "Copy and paste text from the code web pages into the following block, run cell, and save as a CSV file in the `seeds` directory.\n",
    "\n",
    "* [ＪＲＤＢデータコード表](http://www.jrdb.com/program/jrdb_code.txt)\n",
    "* [脚元コード表（2017.02.20）](http://www.jrdb.com/program/ashimoto_code.txt)\n",
    "* [馬具コード表（2017.07.02）](http://www.jrdb.com/program/bagu_code.txt)\n",
    "* [特記コード表（2008.02.23）](http://www.jrdb.com/program/tokki_code.txt)\n",
    "* [系統コード表（2003.05.15）](http://www.jrdb.com/program/keito_code.txt)\n",
    "* [調教コースコード表（2009.10.09）](http://www.jrdb.com/program/cyokyo_course_code.txt)\n",
    "* [追い状態コード表（2008.09.28）](http://www.jrdb.com/program/oi_code.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2d155f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01,流す\n",
      "02,余力あり\n",
      "03,終い抑え\n",
      "04,一杯\n",
      "05,バテる\n",
      "06,伸びる\n",
      "07,テンのみ\n",
      "08,鋭く伸び\n",
      "09,強目\n",
      "10,終い重点\n",
      "11,８分追い\n",
      "12,追って伸\n",
      "13,向正面\n",
      "14,ゲート\n",
      "15,障害練習\n",
      "16,中間軽め\n",
      "17,キリ\n",
      "21,引っ張る\n",
      "22,掛かる\n",
      "23,掛リバテ\n",
      "24,テン掛る\n",
      "25,掛り一杯\n",
      "26,ササル\n",
      "27,ヨレル\n",
      "28,バカつく\n",
      "29,手間取る\n",
      "99,その他\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_text = \"\"\"\n",
    "01      流す\n",
    "02      余力あり\n",
    "03      終い抑え\n",
    "04      一杯\n",
    "05      バテる\n",
    "06      伸びる\n",
    "07      テンのみ\n",
    "08      鋭く伸び\n",
    "09      強目\n",
    "10      終い重点\n",
    "11      ８分追い\n",
    "12      追って伸\n",
    "13      向正面\n",
    "14      ゲート\n",
    "15      障害練習\n",
    "16      中間軽め\n",
    "17      キリ\n",
    "21      引っ張る\n",
    "22      掛かる\n",
    "23      掛リバテ\n",
    "24      テン掛る\n",
    "25      掛り一杯\n",
    "26      ササル\n",
    "27      ヨレル\n",
    "28      バカつく\n",
    "29      手間取る\n",
    "99      その他\n",
    "\"\"\"\n",
    "\n",
    "result = []\n",
    "for line in code_text.strip().splitlines():\n",
    "    result.append(line.strip().split())\n",
    "\n",
    "print(pd.DataFrame(result).to_csv(index=False, header=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
